{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQw7uz_95F_M"
   },
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following cell, if the test version is required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7UlLWrMwGB6O",
    "outputId": "344fee9a-4d9c-407b-cdbf-6f91e32ea221"
   },
   "outputs": [],
   "source": [
    "%pip install biopython\n",
    "%pip install Levenshtein\n",
    "%pip install gensim\n",
    "%pip install pyjarowinkler\n",
    "\n",
    "%pip install --index-url https://test.pypi.org/simple/ --no-deps --force-reinstall corpus_distance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following cell, if the release version is required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install corpus_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reproducibility purposes, the next step is to set two random seeds, as some of the used tools refer to `random.seed`, and others -- to `numpy.random.RandomState`. In addition, in Python 3 it is necessary to set `PYTHONHASHSEED` environment variable to 0 for `FastText` character-based embeddings and `LDA` topic modelling to be reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import RandomState\n",
    "RandomState(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PYTHONHASHSEED=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to configure the logger. This will provide a user with the better understanding of what is going on. By default, the logger provides only warnings (`logging.WARNING` level); to get more verbose information, set to `logging.INFO`. In debug mode it is preferrable to use `logging.DEBUG` level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig(format='%(asctime)s %(name)s:%(levelname)s:%(message)s', level=logging.WARNING, datefmt='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0i4GXBd75O9R"
   },
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is setting the content directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NP9ihxYOyc4P"
   },
   "outputs": [],
   "source": [
    "CONTENT_DIR = \"/home/ilia/coding/Research/datasets/raw_small_corpora/ES\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following step is to create (or set) directory, where the package will store files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from corpus_distance.pipeline import create_and_set_storage_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STORAGE_DIR = \"exp_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_and_set_storage_directory(STORAGE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9JojHkocENSt"
   },
   "source": [
    "Texts (or collections of texts) should be pre-tokenised single strings, (optionally) stored in separate files. Filenames should contain lect name before extension, split by '.'. For example, 'Akimov.Belogornoje.txt', where *Akimov* is a text name, *Belogornoje* is a lect name, and *txt* is an extension.\n",
    "\n",
    "Texts become dictionary keys, and lects names - its values.\n",
    "\n",
    "The `SPLIT` variable regulates the share of the data taken (from `0` to `1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tNc9LBlEyg-d"
   },
   "outputs": [],
   "source": [
    "from corpus_distance.data_preprocessing.data_loading import load_data\n",
    "df = load_data(CONTENT_DIR, SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6fxU4IcFYvV"
   },
   "source": [
    "The next stage is transformation of dictionary into a dataframe of the following format:\n",
    "\n",
    "| index | text | lect |\n",
    "| -------- | ------- |------- |\n",
    "| 0 | text1 | lect1 |\n",
    "| 1 | text2 | lect1 |\n",
    "| 2 | text1 | lect2 |\n",
    "| ... | ... | ... |\n",
    "| m | textN | lectK |\n",
    "\n",
    "*m* here represents the overall number of texts, *K* - the overall number of lects, and *N* is the number of texts in lect *K*.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "XDD3-TwOb83B",
    "outputId": "79458141-8913-4fd1-a0e8-88018e6c080b"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i74_OWtr5kfZ"
   },
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFU3meo4g5gp"
   },
   "source": [
    "Here we get lect names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vw3B0c9No0O4"
   },
   "outputs": [],
   "source": [
    "from corpus_distance.cdutils import get_lects_from_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b8AO985coy4x"
   },
   "outputs": [],
   "source": [
    "lects = get_lects_from_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VF4WEmvqqKty",
    "outputId": "be7b20c5-93c2-45f1-8e1a-1648f35043c9"
   },
   "outputs": [],
   "source": [
    "lects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7X6mFQ70MgWE"
   },
   "source": [
    "## Topic antimodelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3bEPAVTg8RL"
   },
   "source": [
    "Topic antimodelling is used to delete topic words that reflect the features of the texts, and not the language. To enable it, use `TOPIC_NORMALISATION = 'substitute'`, otherwise - `TOPIC_NORMALISATION = 'not_substitute'`. To use topic modelling, use `TOPIC_NORMALISATION = 'topic_words_only'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC_NORMALISATION = 'substitute'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ju4G0yPYM3WV"
   },
   "outputs": [],
   "source": [
    "from corpus_distance.data_preprocessing.topic_modelling import get_topic_words_for_lects, add_topic_modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words = get_topic_words_for_lects(df, lects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without_topics = add_topic_modelling(df, STORAGE_DIR, topic_words, TOPIC_NORMALISATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "FhvqFCrJudRS",
    "outputId": "1e13bf55-5095-40dc-df51-f4299e8d001e"
   },
   "outputs": [],
   "source": [
    "df_without_topics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wnFxN5PR5ny3"
   },
   "source": [
    "## Vectorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUy5revgRJx5"
   },
   "source": [
    "I start with creating a model for representing key properties of the lect:\n",
    "\n",
    "* Its name\n",
    "* Text it contains, lowercased\n",
    "* Its alphabet (with obligatory CLS `^` and EOS `$` symbols)\n",
    "* Amount of entropy of its alphabet\n",
    "* Vector for each given symbol of alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AkXzRHyvsS21"
   },
   "outputs": [],
   "source": [
    "from corpus_distance.data_preprocessing.vectorisation import create_vectors_for_lects, gather_vector_information, FastTextParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hL_YztWKqqI2",
    "outputId": "b8f7d271-4c0f-49c3-8928-805fc9473fdb"
   },
   "outputs": [],
   "source": [
    "vectors_for_lects = create_vectors_for_lects(df_without_topics, STORAGE_DIR, FastTextParams(seed=SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y0OxjYG-sveL"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "APhPLjKg5cUB",
    "outputId": "3c13e62b-fd8e-4b54-f516-59494012841b"
   },
   "outputs": [],
   "source": [
    "pprint(vectors_for_lects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfV9w9IHA9o8"
   },
   "source": [
    "# Date preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7tc-8VxE-nz"
   },
   "source": [
    "The first stage of data preprocessing is splitting tokens into character 3-grams. The character n-grams help to find coinciding sequences more easily, than tokens or token n-grams. Specifically 3-grams help to underscore the exact places where the change is happening, providing minimal left and right context for each symbol within the sequence. Adding special symbols *^* and *$* to the start and the end of each sequence helps to do this for the first and the last symbol of the given sequence as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X0OFGXEbs2nl"
   },
   "outputs": [],
   "source": [
    "from corpus_distance.data_preprocessing.shingle_processing import split_lects_by_n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5C9QTfagb9x5"
   },
   "outputs": [],
   "source": [
    "df_with_n_grams = split_lects_by_n_grams(df_without_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yC_1p94MKCmg"
   },
   "source": [
    "New dataframe is in the following format:\n",
    "\n",
    "| index | lect | n-gram array |\n",
    "| -------- | ------- |------- |\n",
    "| 0 | lect1 | n-grams of lect1 |\n",
    "| 1 | lect1 | n-grams of lect1 |\n",
    "| ... | ... | ... |\n",
    "| k | lectK | n-grams of lect lectK |\n",
    "\n",
    "Here, *k* is overall number of lects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "u8MDVOsHddap",
    "outputId": "95c8f6db-78ba-428f-aacf-03eaf8dd92c0"
   },
   "outputs": [],
   "source": [
    "df_with_n_grams.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1TQvhnPLbom"
   },
   "source": [
    "The next step is to rank n-grams by frequency. The results form *frequency_arranged_n_grams* column of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hSJ60FhttvHP"
   },
   "outputs": [],
   "source": [
    "from corpus_distance.data_preprocessing.frequency_scoring import count_n_grams_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sZHTm7ZEVo5e"
   },
   "outputs": [],
   "source": [
    "df_new = count_n_grams_frequencies(df_with_n_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rwtQ4xwoMoo-"
   },
   "outputs": [],
   "source": [
    "# add information on letter vectors and alphabet entropy to dataframe\n",
    "\n",
    "df_new = gather_vector_information(df_new, vectors_for_lects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "MJNi_14rkPhs",
    "outputId": "53224409-3381-44ab-e629-1040e5642124"
   },
   "outputs": [],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4f2Cvp07CCry"
   },
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tE9YpEaxM0TV"
   },
   "source": [
    "First step is to introduce a measure for hybridisation.\n",
    "\n",
    "One possible measure is scoring Euclidean distance between sum of letter vectors for each n-gram. This results in a loss of order within n-gram, which can yield possible disadvantages (bra === bar), when the measure is used alone; however, when joined with DistRank and Jaro distance, hopefully they yield better results.\n",
    "\n",
    "Optional normalisation includes using alphabet entropy difference, calculated via subtraction of the second alphabet entropy from the first one. This allows to compensate for the cases, when letter from one alphabet may have multiple correspondences in the other, depending on the context. Direct (and not reversed, `1 - X`) measure is better, because the more information one alphabet carries, when contrasted to the other, the more possible one-to-many correspondences there are, the more distortions in vectors there are, the more normalisation is needed.\n",
    "\n",
    "Final normalisation includes traditional split by maximal length of two strings, introduced in Holman et al. (2008)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CxS7yw4avQkI"
   },
   "outputs": [],
   "source": [
    "from corpus_distance.distance_measurement.string_similarity import *\n",
    "from corpus_distance.distance_measurement.hybridisation import HybridisationParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NrFVaCdmvH7M"
   },
   "outputs": [],
   "source": [
    "# assigning global values\n",
    "# group of languages  and its outgroup\n",
    "GROUP = \"East Slavic\"\n",
    "OUTGROUP = \"Zialionka\"\n",
    "\n",
    "# if hybrid metrics aids DistRank\n",
    "HYBRIDISATION = True\n",
    "# if hybrid values join DistRank values in a single array, or they both are\n",
    "# independent values, equally contributing to the final metric\n",
    "HYBRIDISATION_AS_ARRAY = True\n",
    "\n",
    "# if distrank normalisation includes soerensen coefficient\n",
    "SOERENSEN_NORMALISATION = True\n",
    "\n",
    "# choose a metric for hybridisation\n",
    "HYBRID = weighted_jaro_winkler_wrapper\n",
    "\n",
    "# if string similarity measure includes correction by\n",
    "# difference in the alphabet entropies\n",
    "ALPHABET_NORMALISATION = True\n",
    "\n",
    "# metric description\n",
    "METRICS = f\"{GROUP}-{SPLIT}-{TOPIC_NORMALISATION}-DistRank-{SOERENSEN_NORMALISATION}-{HYBRIDISATION}-{HYBRIDISATION_AS_ARRAY}-{HYBRID.__name__}-{ALPHABET_NORMALISATION}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DsK5tj2ewfgL"
   },
   "outputs": [],
   "source": [
    "hybridisation_parameters = HybridisationParameters(HYBRIDISATION, SOERENSEN_NORMALISATION, HYBRIDISATION_AS_ARRAY, HYBRID, ALPHABET_NORMALISATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "EJY_Rn-DiLTz",
    "outputId": "8a1a9fa1-4838-433d-cccd-96b01540b9d5"
   },
   "outputs": [],
   "source": [
    "METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0bdWdnbAv1gE"
   },
   "outputs": [],
   "source": [
    "from corpus_distance.distance_measurement.metrics_pipeline import score_metrics_for_corpus_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5xOwwG54CEJD",
    "outputId": "ddfabedf-72ab-4a34-8919-1bf65bf3539f"
   },
   "outputs": [],
   "source": [
    "# declare arrays\n",
    "# calculate distances for each pair of lects\n",
    "overall_results = score_metrics_for_corpus_dataset(df_new, GROUP, STORAGE_DIR, METRICS, hybridisation_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_matrix = []\n",
    "ordered_lects = list(set(lects))\n",
    "for i in range(len(ordered_lects)):\n",
    "    final_matrix.append([])\n",
    "    for j in range(len(ordered_lects)):\n",
    "        if j < i:\n",
    "            dist =\\\n",
    "                [d[1] for d in overall_results if\\\n",
    "                    set([ordered_lects[i], ordered_lects[j]]) == set([d[0][0], d[0][1]])][0]\n",
    "            final_matrix[i].append(dist)\n",
    "    final_matrix[i].append(0)\n",
    "    for j in range(len(ordered_lects)):\n",
    "        if j > i:\n",
    "            dist =\\\n",
    "                [d[1] for d in overall_results if\\\n",
    "                    set([ordered_lects[i], ordered_lects[j]]) == set([d[0][0], d[0][1]])][0]\n",
    "            final_matrix[i].append(dist)\n",
    "df_res = DataFrame(final_matrix, columns=lects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastnntpy as fn\n",
    "n = fn.run_neighbour_net(df_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fast_nnt_networkx(nx_obj, out_path=\"test/plots/fast_nnt_nx.png\",\n",
    "                           shift=0, node_size=10, font_size=7,\n",
    "                           scale_width_by_weight=False, dpi=300):\n",
    "    # -- data from PyNexus --\n",
    "    labels = {i + shift: s for i, s in nx_obj.get_node_translations()}\n",
    "    pos    = {i + shift: (x, y) for i, x, y in nx_obj.get_node_positions()}\n",
    "    # corrected parsing order: (edge_id, u, v, sid, w)\n",
    "    edges_raw = [ (u + shift, v + shift, w)\n",
    "                  for (_eid, u, v, _sid, w) in nx_obj.get_graph_edges() ]\n",
    "\n",
    "    # only keep edges whose endpoints have positions\n",
    "    edges = [(u, v, w) for (u, v, w) in edges_raw if u in pos and v in pos]\n",
    "    if not edges:\n",
    "        raise ValueError(\"No drawable edges (endpoints missing positions).\")\n",
    "\n",
    "    # -- build graph --\n",
    "    G = nx.Graph()\n",
    "    for u, v, w in edges:\n",
    "        G.add_edge(u, v, weight=w)\n",
    "\n",
    "    # leaves only (degree == 1)\n",
    "    leaves = [n for n, d in G.degree() if d == 1]\n",
    "\n",
    "    # edge widths (optional)\n",
    "    if scale_width_by_weight:\n",
    "        ws = [G[u][v].get(\"weight\", 1.0) for u, v in G.edges()]\n",
    "        wmax = max(ws) if ws else 1.0\n",
    "        widths = [0.5 + 2.5 * (w / wmax) for w in ws]\n",
    "    else:\n",
    "        widths = 0.8\n",
    "\n",
    "    # -- draw (no layout) --\n",
    "    os.makedirs(os.path.dirname(out_path) or \".\", exist_ok=True)\n",
    "    plt.figure(figsize=(8, 8), dpi=dpi)\n",
    "\n",
    "    nx.draw_networkx_edges(G, pos, width=widths, edge_color=\"black\", alpha=0.9)\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=leaves, node_size=node_size, node_color=\"black\")\n",
    "\n",
    "    leaf_labels = {n: labels.get(n, str(n)) for n in leaves}\n",
    "    nx.draw_networkx_labels(G, pos, labels=leaf_labels, font_size=font_size)\n",
    "\n",
    "    plt.axis(\"equal\"); plt.axis(\"off\"); plt.tight_layout(pad=0.02)\n",
    "    plt.savefig(out_path, dpi=dpi, bbox_inches=\"tight\", pad_inches=0.01)\n",
    "    base, _ = os.path.splitext(out_path)\n",
    "    plt.savefig(base + \".svg\", bbox_inches=\"tight\", pad_inches=0.01)\n",
    "    plt.close()\n",
    "    return out_path\n",
    "\n",
    "plot_fast_nnt_networkx(n, out_path=\"1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Labels\")\n",
    "print(len(n.get_labels()))\n",
    "print(\"Splits Records\")\n",
    "print(len(n.get_splits_records()))\n",
    "print(\"Node Translations\")\n",
    "print(len(n.get_node_translations()))\n",
    "print(\"Node Positions\")\n",
    "print(len(n.get_node_positions()))\n",
    "print(\"Graph Edges\")\n",
    "print(len(n.get_graph_edges()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkJIqBNM0tUt"
   },
   "source": [
    "# Clusterisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FX8ByiTMiQiE"
   },
   "source": [
    "The final step is to cluster the lects into groups, and to decide, whether the method works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OsBu1_Og0wD9"
   },
   "outputs": [],
   "source": [
    "from corpus_distance.clusterisation.clusterisation import ClusterisationParameters, clusterise_lects_from_distance_matrix\n",
    "from Bio.Phylo.TreeConstruction import DistanceTreeConstructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m6v_o5hq1LYS"
   },
   "outputs": [],
   "source": [
    "cluster_params = ClusterisationParameters(lects, OUTGROUP, GROUP, METRICS, DistanceTreeConstructor().upgma, STORAGE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5-EL_T3Q7Dwt"
   },
   "outputs": [],
   "source": [
    "clusterise_lects_from_distance_matrix(overall_results, cluster_params)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "lemmatiser-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
